{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lmkmichelle/long-doc-extraction/blob/main/dpr_256.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git remote add origin https://github.com/lmkmichelle/long-doc-extraction.git"
      ],
      "metadata": {
        "id": "Q-S--qJSlXXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade huggingface_hub transformers datasets faiss-cpu"
      ],
      "metadata": {
        "id": "rqxxpUUmo5oY",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "from google.colab import userdata\n",
        "import numpy as np\n",
        "import faiss\n",
        "from datasets import Dataset\n",
        "from transformers import (\n",
        "    DPRContextEncoder, DPRContextEncoderTokenizer,\n",
        "    DPRQuestionEncoder, DPRQuestionEncoderTokenizer,\n",
        "    AutoModelForCausalLM, AutoTokenizer\n",
        ")\n",
        "import torch\n",
        "import re"
      ],
      "metadata": {
        "id": "vUsJEgtMm5JK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "Nz5mE0OnZZO0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!huggingface-cli login"
      ],
      "metadata": {
        "collapsed": true,
        "id": "77szc3HIe9Ko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_grad_enabled(False)\n",
        "\n",
        "ctx_encoder = DPRContextEncoder.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "ctx_tokenizer = DPRContextEncoderTokenizer.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\")\n",
        "\n",
        "q_encoder = DPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n",
        "q_tokenizer = DPRQuestionEncoderTokenizer.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")"
      ],
      "metadata": {
        "collapsed": true,
        "id": "QKe__zahm_ue"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def read_file(file_path):\n",
        "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        return f.read()\n",
        "\n",
        "def clean_text(text):\n",
        "    paragraphs = text.split(\"\\n\\n\")\n",
        "    cleaned_paragraphs = [f\"{i+1}. {p.strip()}\" for i, p in enumerate(paragraphs) if p.strip()]\n",
        "    return cleaned_paragraphs\n",
        "\n",
        "def embed_text_batch(batch, max_length=512):\n",
        "    texts = list(batch[\"text\"])\n",
        "    tokenized = ctx_tokenizer(\n",
        "        texts, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\"\n",
        "    )\n",
        "    with torch.no_grad():\n",
        "        embeddings = ctx_encoder(**tokenized).pooler_output.numpy()\n",
        "    return {\"embeddings\": embeddings.tolist()}\n"
      ],
      "metadata": {
        "id": "JlqhbH3Snh-p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"pandp.txt\"\n",
        "text = read_file(file_path)\n",
        "cleaned_paragraphs = clean_text(text)"
      ],
      "metadata": {
        "id": "TZYc0Y3inkM9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_paragraphs"
      ],
      "metadata": {
        "collapsed": true,
        "id": "2eEeTBWxnl6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 20\n",
        "ds = Dataset.from_dict({\"text\": cleaned_paragraphs, \"paragraph_id\": list(range(1, len(cleaned_paragraphs) + 1))})\n",
        "ds = ds.map(embed_text_batch, batched=True, batch_size=batch_size).add_faiss_index(column=\"embeddings\")"
      ],
      "metadata": {
        "id": "fFS6f-Uyno4F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def search(query, k=3):\n",
        "    question_embedding = q_encoder(**q_tokenizer(query, return_tensors=\"pt\"))[0][0].numpy()\n",
        "    scores, retrieved_examples = ds.get_nearest_examples(\"embeddings\", question_embedding, k=k)\n",
        "    return [(retrieved_examples[\"paragraph_id\"][i], retrieved_examples[\"text\"][i]) for i in range(len(scores))]"
      ],
      "metadata": {
        "id": "zdVYRC0ap5sA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sample query\n",
        "query = \"Who is tolerable?\"\n",
        "results = search(query)\n",
        "\n",
        "for para_id, text in results:\n",
        "    print(f\"Paragraph {para_id}: {text}\\n\")"
      ],
      "metadata": {
        "id": "NhTEM0XBp6_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai google-generativeai"
      ],
      "metadata": {
        "collapsed": true,
        "id": "GgK_DiDTrpaa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def compute_coverage(retrieved_paragraphs, ground_truth_paragraphs):\n",
        "#     return len(set(retrieved_paragraphs) & set(ground_truth_paragraphs)) / len(ground_truth_paragraphs)\n",
        "\n",
        "# def compute_citation(retrieved_paragraphs, ground_truth_paragraphs):\n",
        "#     return len(set(retrieved_paragraphs) & set(ground_truth_paragraphs)) / len(retrieved_paragraphs)"
      ],
      "metadata": {
        "id": "jphf1mx1p901"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ground truth paras // TODO ADD MORE\n",
        "ground_truth_paragraphs = {\"Who is tolerable?\": [242, 244]}\n",
        "\n",
        "retrieved_paragraphs = [para_id for para_id, _ in results]\n",
        "coverage_score = compute_coverage(retrieved_paragraphs, ground_truth_paragraphs[query])\n",
        "citation_score = compute_citation(retrieved_paragraphs, ground_truth_paragraphs[query])\n",
        "\n",
        "print(f\"Coverage Score: {coverage_score:.2f}\")\n",
        "print(f\"Citation Score: {citation_score:.2f}\")\n"
      ],
      "metadata": {
        "id": "7sCsA-s_p_SO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from openai import OpenAI\n",
        "from google import genai\n",
        "\n",
        "google_client = genai.Client(api_key=userdata.get('GEMINI_API'))\n",
        "openai_client = OpenAI(api_key=userdata.get('OPENAI_API_KEY'))\n",
        "\n",
        "query = \"Who is tolerable?\"\n",
        "def get_context(paragraphs):\n",
        "    return \"\\n\\n\".join(paragraphs)\n",
        "\n",
        "# truncate input context because token length\n",
        "N = 300\n",
        "truncated_full_context = get_context(cleaned_paragraphs[:N])\n",
        "\n",
        "dpr_context = get_context([text for _, text in results])\n",
        "oracle_context = get_context([cleaned_paragraphs[i-1] for i in ground_truth_paragraphs[query]])\n",
        "full_context = get_context(cleaned_paragraphs)\n",
        "\n",
        "\n",
        "def generate_gpt4o_answer(query, context):\n",
        "    response = openai_client.chat.completions.create(\n",
        "        model=\"gpt-4o-mini\",\n",
        "        messages=[\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant. Use the context to answer the question and Always cite the paragraph number(s) like this: 36 or 175 in your answer.\"},\n",
        "            {\"role\": \"user\", \"content\": f\"Context:\\n{context}\\n\\nQuestion: {query}\"}\n",
        "        ],\n",
        "        temperature=0.3\n",
        "    )\n",
        "    return response.choices[0].message.content\n",
        "\n",
        "def generate_gemini_answer(query, context):\n",
        "    prompt = f\"Context:\\n{context}\\n\\nQuestion: {query}\\nAnswer clearly and always include paragraph numbers (e.g. 36 or 157) if relevant.\"\n",
        "    response = model.generate_content(prompt)\n",
        "    response = google_client.models.generate_content(\n",
        "        model=\"gemini-1.5-flash\",\n",
        "        contents=prompt,\n",
        "    )\n",
        "    return response.text\n",
        "\n",
        "gpt4o_answers = {\n",
        "    \"dpr\": generate_gpt4o_answer(query, dpr_context),\n",
        "    \"oracle\": generate_gpt4o_answer(query, oracle_context),\n",
        "    \"full\": generate_gpt4o_answer(query, truncated_full_context)\n",
        "}\n",
        "\n",
        "gemini_answers = {\n",
        "    \"dpr\": generate_gemini_answer(query, dpr_context),\n",
        "    \"oracle\": generate_gemini_answer(query, oracle_context),\n",
        "    \"full\": generate_gemini_answer(query, truncated_full_context)\n",
        "}\n",
        "\n"
      ],
      "metadata": {
        "id": "XS_q4X5eaUZr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"GPT-4o-mini FULL:\\n\", gpt4o_answers[\"full\"])\n",
        "print(\"GPT-4o-mini DPR:\\n\", gpt4o_answers[\"dpr\"])\n",
        "print(\"GPT-4o-mini Oracle:\\n\", gpt4o_answers[\"oracle\"])\n",
        "\n",
        "print(\"\\nGemini-1.5-flash FULL:\\n\", gemini_answers[\"full\"])\n",
        "print(\"Gemini-1.5-flash DPR:\\n\", gemini_answers[\"dpr\"])\n",
        "print(\"Gemini-1.5-flash ORACLE:\\n\", gemini_answers[\"oracle\"])"
      ],
      "metadata": {
        "id": "Wn80P6KwdaYm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import re\n",
        "\n",
        "# def extract_cited_paragraphs(answer_text):\n",
        "#     matches = re.findall(r\"(?:paragraph|para)?\\s*\\(?(\\d{1,4})\\)?\", answer_text.lower())\n",
        "#     return list(set(map(int, matches)))\n",
        "\n",
        "# def evaluate_model_output(answer_text, ground_truth_ids):\n",
        "#     cited = extract_cited_paragraphs(answer_text)\n",
        "#     if not cited:\n",
        "#         return 0.0, 0.0\n",
        "#     coverage = compute_coverage(cited, ground_truth_ids)\n",
        "#     citation = compute_citation(cited, ground_truth_ids)\n",
        "#     return coverage, citation\n",
        "\n",
        "# for model_name, outputs in [(\"GPT-4o\", gpt4o_answers), (\"Gemini\", gemini_answers)]:\n",
        "#     print(f\"--- {model_name} ---\")\n",
        "#     for mode in [\"dpr\", \"oracle\", \"full\"]:\n",
        "#         cov, cit = evaluate_model_output(outputs[mode], ground_truth_paragraphs[query])\n",
        "#         print(f\"{mode.upper()} → Coverage: {cov:.2f}, Citation: {cit:.2f}\")\n"
      ],
      "metadata": {
        "id": "hGelIJCnaY2G"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}